{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPiAStrA9B2D",
        "outputId": "b2d6e400-1707-4991-f215-569f87980bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcd1GyB1yh9T",
        "outputId": "725b419e-e4bc-4ddc-bb90-b63f0acf7864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80LyHu2sS6zE",
        "outputId": "9db2581e-8565-4812-a85c-7d6ffdc4cfde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAZj4PHB70nf",
        "outputId": "f0392a45-73ed-499f-a7b8-8384afba3620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating SVM with Word2Vec embeddings...\n",
            "Evaluating KNN with Word2Vec embeddings...\n",
            "Evaluating Decision Tree with Word2Vec embeddings...\n",
            "Evaluating Random Forest with Word2Vec embeddings...\n",
            "Evaluating XGBoost with Word2Vec embeddings...\n",
            "Evaluation Results with Word2Vec embeddings:\n",
            "SVM:\n",
            "Accuracy: 0.5390173410404624\n",
            "Recall: 0.7455540355677155\n",
            "Precision: 0.5466399197592778\n",
            "F1 Score: 0.6307870370370371\n",
            "KNN:\n",
            "Accuracy: 0.5173410404624278\n",
            "Recall: 0.387140902872777\n",
            "Precision: 0.562624254473161\n",
            "F1 Score: 0.4586709886547812\n",
            "Decision Tree:\n",
            "Accuracy: 0.5382947976878613\n",
            "Recall: 0.6990424076607387\n",
            "Precision: 0.5494623655913978\n",
            "F1 Score: 0.6152919927754364\n",
            "Random Forest:\n",
            "Accuracy: 0.5404624277456648\n",
            "Recall: 0.7004103967168263\n",
            "Precision: 0.5511302475780409\n",
            "F1 Score: 0.6168674698795181\n",
            "XGBoost:\n",
            "Accuracy: 0.5404624277456648\n",
            "Recall: 0.7099863201094391\n",
            "Precision: 0.5503711558854719\n",
            "F1 Score: 0.6200716845878136\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/stsa-train.txt', delimiter='\\t')\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/stsa-test.txt', delimiter='\\t')\n",
        "\n",
        "# Extract labels and text from the single column in train_data\n",
        "train_data['Label'] = train_data.iloc[:, 0].apply(lambda x: int(x.split()[0]))\n",
        "train_data['Text'] = train_data.iloc[:, 0].apply(lambda x: ' '.join(x.split()[1:]))\n",
        "\n",
        "# Extract labels and text from the single column in test_data\n",
        "test_data['Label'] = test_data.iloc[:, 0].apply(lambda x: int(x.split()[0]))\n",
        "test_data['Text'] = test_data.iloc[:, 0].apply(lambda x: ' '.join(x.split()[1:]))\n",
        "\n",
        "# Drop the original columns\n",
        "train_data.drop(columns=[train_data.columns[0]], inplace=True)\n",
        "test_data.drop(columns=[test_data.columns[0]], inplace=True)\n",
        "\n",
        "# Split the training data into features and labels\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data['Text'].values,\n",
        "                                                  train_data['Label'].values,\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42)\n",
        "\n",
        "# Split the test data into features and labels\n",
        "X_test = test_data['Text'].values\n",
        "y_test = test_data['Label'].values\n",
        "\n",
        "# Initialize Word2Vec model\n",
        "word2vec_model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Define a function to preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # You can implement your text preprocessing steps here\n",
        "    # For now, let's just return the text as is\n",
        "    return text\n",
        "\n",
        "# Perform preprocessing\n",
        "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
        "X_val_preprocessed = [preprocess_text(text) for text in X_val]\n",
        "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model.build_vocab(X_train_preprocessed)\n",
        "word2vec_model.train(X_train_preprocessed, total_examples=word2vec_model.corpus_count, epochs=10)\n",
        "\n",
        "# Extract Word2Vec embeddings for train, validation, and test data\n",
        "X_train_word2vec = []\n",
        "X_val_word2vec = []\n",
        "X_test_word2vec = []\n",
        "\n",
        "for text in X_train_preprocessed:\n",
        "    words = text.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in word2vec_model.wv:\n",
        "            embeddings.append(word2vec_model.wv[word])\n",
        "    if embeddings:\n",
        "        X_train_word2vec.append(np.mean(embeddings, axis=0))\n",
        "    else:\n",
        "        X_train_word2vec.append(np.zeros(100))\n",
        "\n",
        "for text in X_val_preprocessed:\n",
        "    words = text.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in word2vec_model.wv:\n",
        "            embeddings.append(word2vec_model.wv[word])\n",
        "    if embeddings:\n",
        "        X_val_word2vec.append(np.mean(embeddings, axis=0))\n",
        "    else:\n",
        "        X_val_word2vec.append(np.zeros(100))\n",
        "\n",
        "for text in X_test_preprocessed:\n",
        "    words = text.split()\n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        if word in word2vec_model.wv:\n",
        "            embeddings.append(word2vec_model.wv[word])\n",
        "    if embeddings:\n",
        "        X_test_word2vec.append(np.mean(embeddings, axis=0))\n",
        "    else:\n",
        "        X_test_word2vec.append(np.zeros(100))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_word2vec = np.array(X_train_word2vec)\n",
        "X_val_word2vec = np.array(X_val_word2vec)\n",
        "X_test_word2vec = np.array(X_test_word2vec)\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}\n",
        "\n",
        "# Initialize evaluation metrics\n",
        "evaluation_metrics = {\n",
        "    'Accuracy': accuracy_score,\n",
        "    'Recall': recall_score,\n",
        "    'Precision': precision_score,\n",
        "    'F1 Score': f1_score\n",
        "}\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "results_word2vec = {}\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"Evaluating {clf_name} with Word2Vec embeddings...\")\n",
        "    clf.fit(X_train_word2vec, y_train)\n",
        "    y_pred = clf.predict(X_val_word2vec)\n",
        "    clf_results = {}\n",
        "    for metric_name, metric_func in evaluation_metrics.items():\n",
        "        clf_results[metric_name] = metric_func(y_val, y_pred)\n",
        "    results_word2vec[clf_name] = clf_results\n",
        "\n",
        "# BERT Classifier\n",
        "# The BERT part is unchanged from the previous version\n",
        "\n",
        "# Print results\n",
        "print(\"Evaluation Results with Word2Vec embeddings:\")\n",
        "for clf_name, clf_result in results_word2vec.items():\n",
        "    print(clf_name + \":\")\n",
        "    for metric_name, metric_value in clf_result.items():\n",
        "        print(f\"{metric_name}: {metric_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoQX5s4O70nf",
        "outputId": "5ec1a802-f54c-4431-d8ed-37f9b45a9c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means Clustering Results:\n",
            "[2 2 2 ... 2 2 2]\n",
            "\n",
            "DBSCAN Clustering Results:\n",
            "[-1 -1 -1 ... -1 -1 -1]\n",
            "\n",
            "Hierarchical Clustering Results:\n",
            "[0 0 0 ... 0 0 0]\n",
            "\n",
            "Word2Vec Clustering Results:\n",
            "[1 1 0 ... 2 2 2]\n",
            "\n",
            "BERT Clustering Results:\n",
            "[3 3 2 ... 3 1 0]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Loading the sample of the dataset to reduce memory and computational requirements\n",
        "# Reading the first 10,000 rows as a sample\n",
        "sample_size = 10000\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Amazon_Unlocked_Mobile.csv\", nrows=sample_size)\n",
        "\n",
        "# Drop rows with missing values (NaN) in the 'Reviews' column\n",
        "data.dropna(subset=['Reviews'], inplace=True)\n",
        "\n",
        "# Preprocess the text data (e.g., remove stopwords, tokenize, lowercase, etc.)\n",
        "preprocessed_text = []\n",
        "\n",
        "for review in data['Reviews']:\n",
        "    # Tokenize the review into words\n",
        "    tokens = nltk.word_tokenize(review)\n",
        "    # Optionally, perform additional preprocessing steps such as removing stopwords, punctuation, etc.\n",
        "    preprocessed_text.append(tokens)\n",
        "\n",
        "# 1. K-means clustering\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Reviews'].astype(str))\n",
        "kmeans_model = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_clusters = kmeans_model.fit_predict(tfidf_matrix)\n",
        "\n",
        "# 2. DBSCAN clustering\n",
        "dbscan_model = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_clusters = dbscan_model.fit_predict(tfidf_matrix)\n",
        "\n",
        "# 3. Hierarchical clustering\n",
        "hierarchical_model = AgglomerativeClustering(n_clusters=5)\n",
        "hierarchical_clusters = hierarchical_model.fit_predict(tfidf_matrix.toarray())\n",
        "\n",
        "# 4. Word2Vec clustering\n",
        "word2vec_model = Word2Vec(preprocessed_text, vector_size=100, window=5, min_count=5, workers=4)\n",
        "word2vec_kmeans_model = KMeans(n_clusters=5, random_state=42)\n",
        "word2vec_clusters = word2vec_kmeans_model.fit_predict(word2vec_model.wv.vectors)\n",
        "\n",
        "# 5. BERT clustering\n",
        "sentences = data['Reviews'].tolist()\n",
        "sentence_transformer_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "bert_embeddings = sentence_transformer_model.encode(sentences)\n",
        "bert_kmeans_model = KMeans(n_clusters=5, random_state=42)\n",
        "bert_clusters = bert_kmeans_model.fit_predict(bert_embeddings)\n",
        "\n",
        "print(\"K-means Clustering Results:\")\n",
        "print(kmeans_clusters)\n",
        "\n",
        "print(\"\\nDBSCAN Clustering Results:\")\n",
        "print(dbscan_clusters)\n",
        "\n",
        "print(\"\\nHierarchical Clustering Results:\")\n",
        "print(hierarchical_clusters)\n",
        "\n",
        "print(\"\\nWord2Vec Clustering Results:\")\n",
        "print(word2vec_clusters)\n",
        "\n",
        "print(\"\\nBERT Clustering Results:\")\n",
        "print(bert_clusters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRijW2aLGONl"
      },
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIYCj5qyGfSL"
      },
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".\n",
        "Data points are grouped together according to the clustering results obtained from Word2Vec, DBSCAN, Hierarchical clustering, K-means, and BERT. While Hierarchical clustering arranges clusters in a tree-like structure, DBSCAN searches for dense areas, Word2Vec/BERT groups data based on semantic meanings. K-means finds clusters with distinct boundaries. Finding patterns and similarities in the data is made easier by the distinct viewpoints that each approach offers. What you hope to learn from the data will determine which approach is best.\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4291b421-1cc1-493b-88a9-b026a115e824"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBit hard to work on this assignment. Time killing. Took lot of time to execute.\\nIt's not easy to work on these for a very less time. \\nThe dataset for the 2nd question contains 4L data which is very much time consuming to execute \\nand the processing time will be also more. And with that dataset BERT algorithm nearly takes hours to execute and \\nruntime is crashing. So, I used sample size\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Bit hard to work on this assignment. Time killing. Took lot of time to execute.\n",
        "It's not easy to work on these for a very less time.\n",
        "The dataset for the 2nd question contains 4L data which is very much time consuming to execute\n",
        "and the processing time will be also more. And with that dataset BERT algorithm nearly takes hours to execute and\n",
        "runtime is crashing. So, I used sample size\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}